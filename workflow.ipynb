{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Workshop Outline\n",
    "\n",
    " # Local RAG System\n",
    "\n",
    "\n",
    "\n",
    " <img src=\"images/ragflowchart.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "\n",
    " ## • Embedding of text\n",
    "\n",
    " - Read/Load Document (txt, pdf, etc.)\n",
    "\n",
    " - Split text into chunks considering embedding limit and basic context\n",
    "\n",
    " - Encode text into vector\n",
    "\n",
    " - Find Similarity\n",
    "\n",
    " ## &bull; Vector store\n",
    "\n",
    " - Using a Vector Database\n",
    "\n",
    " - Make a collection\n",
    "\n",
    " - Query collection. Find related text.\n",
    "\n",
    " ## &bull; Query LLM with contextual data\n",
    "\n",
    " -----------------------------------------------------------------------------\n",
    "\n",
    " ## Hands on...\n",
    "\n",
    " #### &bull; **Embedding of text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load Embedding model\n",
    "# https://www.sbert.net/\n",
    "# pip3 install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Sample Text\n",
    "\n",
    "texts = [\n",
    "    \"Bali's beautiful beaches and rich culture stands out as a fantastic travel destination.\",\n",
    "    \"Pizza in Rome is famous for its thin crust, fresh ingredients and wood-fired ovens.\",\n",
    "    \"Graphics processing units (GPU) have become an essential foundation for artificial intelligence.\",\n",
    "    \"Newton's laws of motion transformed our understanding of physics.\",\n",
    "    \"The French Revolution played a crucial role in shaping contemporary France.\",\n",
    "    \"Maintaining good health requires regular exercise, balanced diet and quality sleep.\",\n",
    "    \"Dali's surrealistic artworks, like 'The Persistence of Memory,' captivate audiences with their dreamlike imagery and imaginative brilliance\",\n",
    "    \"Global warming threatens the planet's ecosystems and wildlife.\",\n",
    "    \"The KI-Servicezentrum Berlin-Brandenburg offers services such as consulting, workshops, MOOCs and computer resources.\",\n",
    "    \"Django Reinhardt's jazz compositions are celebrated for their captivating melodies and innovative guitar work..\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Encode texts\n",
    "\n",
    "text_embeddings = model.encode(texts)\n",
    "\n",
    "print(\"Embeddings type:\", type(text_embeddings))\n",
    "print(\"Embeddings Matrix shape:\", text_embeddings.shape)\n",
    "\n",
    "# Note: \"all-MiniLM-L6-v2\" encodes texts up to 256 words. It’ll truncate any text longer than this.\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### &bull; **Check Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Storing Embeddings in a simple dict\n",
    "\n",
    "text_embs_dict = dict(zip(texts, list(text_embeddings)))\n",
    "# key: text, value: numpy array with embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define similarity metric\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cos_sim(x, y):\n",
    "    return dot(x, y) / (norm(x) * norm(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Check similarities\n",
    "\n",
    "test_text = \"I really need some vacations\"\n",
    "emb_test_text = model.encode(test_text)\n",
    "\n",
    "print(f\"\\nCosine similarities for: '{test_text}'\\n\")\n",
    "for k, v in text_embs_dict.items():\n",
    "    print(k, round(cos_sim(emb_test_text, v), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **To do:**\n",
    "\n",
    "\n",
    "\n",
    " - Experiment with different sentences and compare similarities.\n",
    "\n",
    "\n",
    "\n",
    " - Check alternative sentence-transformers. Read Model Cards. Compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### &bull; **Using a Vector Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.Client()\n",
    "# client = chromadb.PersistentClient(path=\"chroma_data/\")\n",
    "# For a ChromaDB instance on the disk\n",
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Init collection\n",
    "\n",
    "COLLECTION_NAME = \"demo_docs\"\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_func,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create collection\n",
    "# (Adding metadata. Optional)\n",
    "\n",
    "topics = [\n",
    "    \"travel\",\n",
    "    \"food\",\n",
    "    \"technology\",\n",
    "    \"science\",\n",
    "    \"history\",\n",
    "    \"health\",\n",
    "    \"painting\",\n",
    "    \"climate change\",\n",
    "    \"business\",\n",
    "    \"music\",\n",
    "]\n",
    "\n",
    "collection.add(\n",
    "    documents=texts,\n",
    "    ids=[f\"id{i}\" for i in range(len(texts))],\n",
    "    metadatas=[{\"topic\": topic} for topic in topics],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Quering the Database\n",
    "\n",
    "query1 = \"I am looking for something to eat\"\n",
    "query2 = \"What's going on in the world?\"\n",
    "query3 = \"Great jazz player\"\n",
    "\n",
    "queries = [query1, query2, query3]\n",
    "\n",
    "query_results = collection.query(\n",
    "    query_texts=queries,  # list of strings or just one element (string)\n",
    "    n_results=2,\n",
    ")\n",
    "print(\"Query dict keys:\")\n",
    "print(query_results.keys())\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    print(\"\\nQuery:\", queries[i])\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    for j in range(len(query_results[\"ids\"][i])):\n",
    "        print(\"id:\", query_results[\"ids\"][i][j])\n",
    "        print(\"Text:\", query_results[\"documents\"][i][j])\n",
    "        print(\"Distance:\", round(query_results[\"distances\"][i][j], 2))\n",
    "        print(\"Metadata:\", query_results[\"metadatas\"][i][j])\n",
    "\n",
    "    print(80 * \"-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### &bull; **Running a Large Language Model locally with Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import requests, json\n",
    "\n",
    "# $ ollama serve\n",
    "BASEURL = \"http://localhost:11434/api\"\n",
    "MODEL = 'mistral'\n",
    "\n",
    "\n",
    "def generate(prompt, context=[], top_k=5, top_p=0.9, temp=0.5):\n",
    "    url = BASEURL + \"/generate\"\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": MODEL,\n",
    "        \"stream\": False,\n",
    "        \"context\": context,\n",
    "        \"options\": {\"temperature\": temp, \"top_p\": top_p, \"top_k\": top_k},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.post(url, json=data)\n",
    "        response_dic = json.loads(r.text)\n",
    "        return response_dic.get('response', ''), response_dic.get('context', '')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "llm_response, _ = generate(\"Hi, who are you\", top_k=10, top_p=0.9, temp=0.5)\n",
    "print(llm_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### &bull; **Make a simple local LLM chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "user_input = \"Hi. who are you?\"\n",
    "ollama_context = []\n",
    "print(f\"Start chatting with {MODEL} model (Press q to quit)\\n\")\n",
    "while user_input != \"q\":\n",
    "    bot_response, ollama_context = generate(\n",
    "        user_input, context=ollama_context, top_k=10, top_p=0.9, temp=0.5\n",
    "    )\n",
    "    print(\"Model message:\")\n",
    "    print(bot_response)\n",
    "    user_input = input(\"\\nYour prompt: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **To do:**\n",
    "\n",
    "\n",
    "\n",
    " - Experiment with different input parameters, arguments, prompts and compare results.\n",
    "\n",
    " - Try different models (e.g. mistral, llama2, gemma, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----------------------------------------------------------------------------\n",
    "\n",
    " ## Integration\n",
    "\n",
    " #### **Integrate the components for a RAG System.**\n",
    "\n",
    " ### References:\n",
    "\n",
    " #### &bull; **Text embedding: Sentence Bert**\n",
    "\n",
    " https://www.sbert.net/\n",
    "\n",
    "\n",
    "\n",
    " Sample Sentence Transformers:\n",
    "\n",
    "\n",
    "\n",
    " https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "\n",
    "\n",
    " https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n",
    "\n",
    "\n",
    "\n",
    " https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
    "\n",
    "\n",
    "\n",
    " Check and compare\n",
    "\n",
    " #### &bull; **Vector Database: ChromaDB**\n",
    "\n",
    " https://docs.trychroma.com/\n",
    "\n",
    " #### &bull; **Local LLM: ollama**\n",
    "\n",
    " https://ollama.ai/\n",
    "\n",
    " #### &bull; **Frontend: Gradio**\n",
    "\n",
    " https://www.gradio.app/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
